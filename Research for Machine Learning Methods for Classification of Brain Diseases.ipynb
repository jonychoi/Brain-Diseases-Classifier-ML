{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dLzQ5B_vUgX"
      },
      "source": [
        "#**Research for Classification of Brain disease between 'Alzheimer's Disease', 'Mild Coginitive Impariment', 'Cognitive Normal' with various methods of Machine Learning.**\n",
        "###**Introduction to Brain and Machine Learning, 2020-2**\n",
        "###**Prof. Suk. , Korea University**\n",
        "###**All codes are written by John Leo**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfQMMuUpOpWP"
      },
      "source": [
        "#**Used Libraries for the project**\n",
        "---\n",
        "###**Processing the dataframe**\n",
        "1. **math**: For detecting the nan value\n",
        "2. **pandas**: For processing total dataframe\n",
        "3. **statistics**: For getting median value from the array\n",
        "4. **numpy**: To create array\n",
        "\n",
        "###**Constructing the model**\n",
        "1. **Scikit Learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXWSbPahvOTd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/train_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omJMmGB5DySN"
      },
      "source": [
        "#**PreProcessing of Input Features: Morphological phenotypes**\n",
        "---\n",
        "#**Classes**\n",
        "\n",
        "###**::MissingTask(rawData):**\n",
        "\n",
        "This class fills the missing features(nan) to the specific value, and return the dataframe.\n",
        "Each missing feature cell is filled with the mean, median and zero of **the existing values of the columns containing the missing features** by each method (Mean, Median, Zero).\n",
        "\n",
        "**Methods**\n",
        "\n",
        "- **Mean()** : Fill the missing values with the Mean of the **exist values from the missing value column**\n",
        ">\n",
        "- **Zero()** : Fill the missing values with Zero\n",
        ">\n",
        "- **Median()** : Fill the missing values with Median of the **exist values from the missng value column**\n",
        "\n",
        "---\n",
        "###**::FullData(data)::**\n",
        "\n",
        "This Class is consist of methods for seperating the data by Cortical Volume (E-BV), Average Thickness (BW-EN), and the dataframe itself.\n",
        "\n",
        "**Attributes**\n",
        "\n",
        "- **DiagnosisGroup** : DataFrame of the Diagnosis columns of the FullData\n",
        ">\n",
        "- **CorticalVolume**: DataFrame of the Cortical Volume columns of the FullData\n",
        ">\n",
        "- **AverageThickness**: DataFrame of the Average Thickness columns of the Fulldata\n",
        "\n",
        "**Methods**\n",
        "- **getDiagnosisGroup()** : Return the Diagnosis Columns from of Full Data\n",
        ">\n",
        "- **getCorticalVolume()** : Return the Cortical Volume Columns of Full Data\n",
        ">\n",
        "- **getAverageThickness()** : Return the Cortical Volume Columns of Full Data\n",
        ">\n",
        "- **getVolumeArray(type)**: Return the Array of the Volumes by the type.\n",
        "> **type == \"All\"** => return the all volumes array(CV, AT).\n",
        "> ##### **type == \"CV\"** => return the Cortical Volume array\n",
        "> ##### **type == \"AT\"** => return the Average Thickness array.\n",
        "- **getScoreArray(type)**: Return the Array of the Scores by the type\n",
        "> ##### **type == \"All\"** => return the all scores array (ADAS11, ADAS13, MMSE\n",
        "> ##### **type == \"Question\"** => return the ADAS11, ADAS13 array\n",
        "> ##### **type == \"MMSE\"** => return the MMSE array.\n",
        "- **getDiagnosisArray()**: Return the Diagnosis Column of Full Data.\n",
        ">\n",
        "- **getVolumeArrayByColumns()**: Return the Array by columns\n",
        "\n",
        "---\n",
        "###**::ClassData(taskedData, diagnosisIndex)::**\n",
        "This class seprates the full data to the each diagnosis groups, which are Cognitive Normal (0), Mild Cogintive Impariment (1), Alzheimer's Disease (2). Each class has the method for getting the full dataframe, dataframe for cortical volume, dataframe for average thickness, and the mean of the each cortical volume and average thickness.\n",
        "\n",
        "**Methods**\n",
        "- **getData()** : Get the full dataframe of the class of the diagnosis group\n",
        ">\n",
        "- **getCorticalVolume()** : Get the Cortical Volume columns of the class\n",
        ">\n",
        "- **getAverageThickness()** : Get the Average Thickness columns of the class\n",
        ">\n",
        "- **getColumnArray(type)** : Get the Array of Mean or Median of each columns\n",
        ">\n",
        "- **getMeanOfCV()** : Get the Mean of the Cortical Volume from the whole values of the class\n",
        ">\n",
        "- **getMeanOfAT()** : Get the Mean of the Average Thickness from the whole values of the class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DtRGR7Evy-I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "9800c93b-836a-4c17-a117-418b8b7065e2"
      },
      "source": [
        "class MissingTask:\n",
        "  def __init__(self, rawdata):\n",
        "    self.data = rawdata.copy()\n",
        "    self.columns = df.columns[4:]\n",
        "  \n",
        "  def Mean(self):\n",
        "    dictionary = {}\n",
        "    for column in self.columns:\n",
        "      average = 0\n",
        "      sum = 0\n",
        "      count = 0\n",
        "      for volume in self.data[column]:\n",
        "        if math.isnan(volume):\n",
        "          volume = 0\n",
        "        else:\n",
        "          count = count + 1\n",
        "        sum = sum + volume\n",
        "        average = sum / count\n",
        "        dictionary[column] = average\n",
        "\n",
        "    for dic in dictionary:\n",
        "      for index, volume in enumerate(self.data[dic]):\n",
        "        if math.isnan(volume):\n",
        "          self.data[dic][index] = dictionary[dic]\n",
        "\n",
        "    return self.data\n",
        "\n",
        "\n",
        "  def Zero(self):\n",
        "    for column in self.columns:\n",
        "      for index, volume in enumerate(self.data[column]):\n",
        "        if math.isnan(volume):\n",
        "          self.data[column][index] = 0\n",
        "\n",
        "    return self.data\n",
        "\n",
        "  def Median(self):\n",
        "    dictionary = {}\n",
        "    for column in self.columns:\n",
        "      array = []\n",
        "      for volume in self.data[column]:\n",
        "        if math.isnan(volume) is False:\n",
        "          array.append(volume)\n",
        "      median = statistics.median(array)\n",
        "      dictionary[column] = median\n",
        "\n",
        "    for dic in dictionary:\n",
        "      for index, volume in enumerate(self.data[dic]):\n",
        "        if math.isnan(volume):\n",
        "          self.data[dic][index] = dictionary[dic]\n",
        "\n",
        "    return self.data\n",
        "\n",
        "  @staticmethod\n",
        "  def CheckIfMeanRight():\n",
        "    sum = 0\n",
        "    count = 0\n",
        "    notcount = 0\n",
        "    for volume in df['ST123CV']:\n",
        "      if math.isnan(volume):\n",
        "        volume = 0\n",
        "        notcount = notcount + 1\n",
        "      else:\n",
        "        count = count + 1\n",
        "      sum = sum + volume\n",
        "    average = sum / count\n",
        "    print(len(df), count, notcount, count+notcount)\n",
        "    \n",
        "\n",
        "class FullData:\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.DiagnosisGroup = self.getDiagnosisGroup()\n",
        "    self.CorticalVolume = self.getCorticalVolume()\n",
        "    self.AverageThickness = self.getAverageThickness()\n",
        "\n",
        "  def getDiagnosisGroup(self):\n",
        "    return self.data['DX_bl']\n",
        "\n",
        "  def getCorticalVolume(self):\n",
        "    return pd.concat([self.DiagnosisGroup, self.data.iloc[:,4:74]], axis=1)\n",
        "\n",
        "  def getAverageThickness(self):\n",
        "    return pd.concat([self.DiagnosisGroup, self.data.iloc[:,74:]], axis=1)\n",
        "\n",
        "  def getVolumeArray(self, type):\n",
        "    if type is 'CV':\n",
        "      return np.array(self.CorticalVolume.iloc[:,1:])\n",
        "    elif type is 'AT':\n",
        "      return np.array(self.AverageThickness.iloc[:,1:])\n",
        "    elif type is 'All':\n",
        "      return np.array(self.data.iloc[:,4:])\n",
        "  \n",
        "  def getScoreArray(self, type):\n",
        "    if type is 'All':\n",
        "     return np.array(self.data.iloc[:,1:4])\n",
        "    elif type is 'Question':\n",
        "      return np.array(self.data.iloc[:,1:3])\n",
        "    elif type is 'Mini':\n",
        "      return np.array(self.data.iloc[:,3])\n",
        "                      \n",
        "  def getDiagnosisArray(self):\n",
        "    return np.array(self.DiagnosisGroup)\n",
        "\n",
        "  def getVolumeArrayByColumns(self):\n",
        "    data = self.data.iloc[:10,4:]\n",
        "    columns = data.columns\n",
        "    \n",
        "    array = []\n",
        "\n",
        "    for column in columns:\n",
        "      inarray = []\n",
        "      for volume in data[column]:\n",
        "        inarray.append(volume)\n",
        "      array.append(inarray)\n",
        "\n",
        "    return array\n",
        "\n",
        "\n",
        "class ClassData:\n",
        "  def __init__(self, taskedData, diagnosisIndex):\n",
        "    self.taskedData = taskedData\n",
        "    self.diagnosisIndex = diagnosisIndex\n",
        "    self.data = self.taskedData[taskedData['DX_bl'] == diagnosisIndex]\n",
        "    self.CorticalVolume = self.getCorticalVolume()\n",
        "    self.AverageThickness = self.getAverageThickness()\n",
        "\n",
        "  def getData(self):\n",
        "    return self.data\n",
        "  \n",
        "  def getCorticalVolume(self):\n",
        "    cortical = self.data.iloc[:, 4:74]\n",
        "    diagnosisColumn = self.data['DX_bl']\n",
        "    return pd.concat([diagnosisColumn, cortical], axis =1)\n",
        "\n",
        "  def getAverageThickness(self):\n",
        "    average = self.data.iloc[:, 74:]\n",
        "    diagnosisColumn = self.data['DX_bl']\n",
        "    return pd.concat([diagnosisColumn, average], axis =1)\n",
        "\n",
        "  def getColumnArray(self, volumeType, type):\n",
        "    if volumeType is 'CV':\n",
        "      data = self.CorticalVolume\n",
        "    else:\n",
        "      data = self.AverageThickness\n",
        "    columns = data.columns[1:]\n",
        "    array = []\n",
        "    if type is 'Mean':\n",
        "      for column in columns:\n",
        "        sum = 0\n",
        "        for volume in data[column]:\n",
        "          sum = sum + volume\n",
        "        mean = sum / len(data[column])\n",
        "        array.append(mean)\n",
        "      return array, data.columns[1:]\n",
        "    else:\n",
        "      for column in columns:\n",
        "        median = statistics.median(data[column])\n",
        "        array.append(median)\n",
        "      return array, data.columns[1:]\n",
        "\n",
        "\n",
        "\n",
        "  def MeanOfCV(self):\n",
        "    columns = self.CorticalVolume.columns[1:]\n",
        "    totalcellnumber = len(columns) * len(self.CorticalVolume)\n",
        "    total = 0\n",
        "    for column in columns:\n",
        "      sum = 0\n",
        "      for volume in self.CorticalVolume[column]:\n",
        "        sum = sum + volume\n",
        "      total = total + sum\n",
        "    return total / totalcellnumber\n",
        "\n",
        "  def MeanOfAT(self):\n",
        "    columns = self.AverageThickness.columns[1:]\n",
        "    totalcellnumber = len(columns) * len(self.AverageThickness)\n",
        "    total = 0\n",
        "    for column in columns:\n",
        "      sum = 0\n",
        "      for volume in self.AverageThickness[column]:\n",
        "        sum = sum + volume\n",
        "      total = total + sum\n",
        "    return total / totalcellnumber\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the data by the selection of the method of processing missing features of MISSING TASK CLASS\n",
        "# Here the tasked data is defined with the Mean Processing missing features.\n",
        "\n",
        "Data = FullData(MissingTask(df).Mean())\n",
        "\n",
        "Data.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DX_bl</th>\n",
              "      <th>ADAS11</th>\n",
              "      <th>ADAS13</th>\n",
              "      <th>MMSE</th>\n",
              "      <th>ST102CV</th>\n",
              "      <th>ST103CV</th>\n",
              "      <th>ST104CV</th>\n",
              "      <th>ST105CV</th>\n",
              "      <th>ST106CV</th>\n",
              "      <th>ST107CV</th>\n",
              "      <th>ST108CV</th>\n",
              "      <th>ST109CV</th>\n",
              "      <th>ST110CV</th>\n",
              "      <th>ST111CV</th>\n",
              "      <th>ST113CV</th>\n",
              "      <th>ST114CV</th>\n",
              "      <th>ST115CV</th>\n",
              "      <th>ST116CV</th>\n",
              "      <th>ST117CV</th>\n",
              "      <th>ST118CV</th>\n",
              "      <th>ST119CV</th>\n",
              "      <th>ST121CV</th>\n",
              "      <th>ST123CV</th>\n",
              "      <th>ST129CV</th>\n",
              "      <th>ST130CV</th>\n",
              "      <th>ST13CV</th>\n",
              "      <th>ST14CV</th>\n",
              "      <th>ST15CV</th>\n",
              "      <th>ST23CV</th>\n",
              "      <th>ST24CV</th>\n",
              "      <th>ST25CV</th>\n",
              "      <th>ST26CV</th>\n",
              "      <th>ST31CV</th>\n",
              "      <th>ST32CV</th>\n",
              "      <th>ST34CV</th>\n",
              "      <th>ST35CV</th>\n",
              "      <th>ST36CV</th>\n",
              "      <th>ST38CV</th>\n",
              "      <th>ST39CV</th>\n",
              "      <th>ST40CV</th>\n",
              "      <th>...</th>\n",
              "      <th>ST34TA</th>\n",
              "      <th>ST35TA</th>\n",
              "      <th>ST36TA</th>\n",
              "      <th>ST38TA</th>\n",
              "      <th>ST39TA</th>\n",
              "      <th>ST40TA</th>\n",
              "      <th>ST43TA</th>\n",
              "      <th>ST44TA</th>\n",
              "      <th>ST45TA</th>\n",
              "      <th>ST46TA</th>\n",
              "      <th>ST47TA</th>\n",
              "      <th>ST48TA</th>\n",
              "      <th>ST49TA</th>\n",
              "      <th>ST50TA</th>\n",
              "      <th>ST51TA</th>\n",
              "      <th>ST52TA</th>\n",
              "      <th>ST54TA</th>\n",
              "      <th>ST55TA</th>\n",
              "      <th>ST56TA</th>\n",
              "      <th>ST57TA</th>\n",
              "      <th>ST58TA</th>\n",
              "      <th>ST59TA</th>\n",
              "      <th>ST60TA</th>\n",
              "      <th>ST62TA</th>\n",
              "      <th>ST64TA</th>\n",
              "      <th>ST72TA</th>\n",
              "      <th>ST73TA</th>\n",
              "      <th>ST74TA</th>\n",
              "      <th>ST82TA</th>\n",
              "      <th>ST83TA</th>\n",
              "      <th>ST84TA</th>\n",
              "      <th>ST85TA</th>\n",
              "      <th>ST90TA</th>\n",
              "      <th>ST91TA</th>\n",
              "      <th>ST93TA</th>\n",
              "      <th>ST94TA</th>\n",
              "      <th>ST95TA</th>\n",
              "      <th>ST97TA</th>\n",
              "      <th>ST98TA</th>\n",
              "      <th>ST99TA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9.00</td>\n",
              "      <td>14.00</td>\n",
              "      <td>27</td>\n",
              "      <td>3295</td>\n",
              "      <td>1760</td>\n",
              "      <td>2957</td>\n",
              "      <td>1975</td>\n",
              "      <td>3270</td>\n",
              "      <td>1871</td>\n",
              "      <td>9504</td>\n",
              "      <td>3219</td>\n",
              "      <td>12683</td>\n",
              "      <td>8167</td>\n",
              "      <td>1956</td>\n",
              "      <td>15106</td>\n",
              "      <td>18296</td>\n",
              "      <td>12517</td>\n",
              "      <td>11914</td>\n",
              "      <td>9771</td>\n",
              "      <td>2024</td>\n",
              "      <td>1221</td>\n",
              "      <td>6627.000000</td>\n",
              "      <td>6245</td>\n",
              "      <td>5844</td>\n",
              "      <td>2361</td>\n",
              "      <td>2161</td>\n",
              "      <td>4782</td>\n",
              "      <td>2116</td>\n",
              "      <td>1757.0</td>\n",
              "      <td>723</td>\n",
              "      <td>6714</td>\n",
              "      <td>11308</td>\n",
              "      <td>10841</td>\n",
              "      <td>1669</td>\n",
              "      <td>8168</td>\n",
              "      <td>6690</td>\n",
              "      <td>6040</td>\n",
              "      <td>3858</td>\n",
              "      <td>11530</td>\n",
              "      <td>...</td>\n",
              "      <td>2.098</td>\n",
              "      <td>1.965</td>\n",
              "      <td>2.666</td>\n",
              "      <td>1.690</td>\n",
              "      <td>2.328</td>\n",
              "      <td>2.636</td>\n",
              "      <td>2.159</td>\n",
              "      <td>2.354</td>\n",
              "      <td>2.174</td>\n",
              "      <td>2.331</td>\n",
              "      <td>2.335</td>\n",
              "      <td>1.353</td>\n",
              "      <td>1.757</td>\n",
              "      <td>2.270</td>\n",
              "      <td>2.189</td>\n",
              "      <td>2.099</td>\n",
              "      <td>2.695</td>\n",
              "      <td>2.112</td>\n",
              "      <td>2.537</td>\n",
              "      <td>2.004</td>\n",
              "      <td>2.698</td>\n",
              "      <td>2.445</td>\n",
              "      <td>3.461</td>\n",
              "      <td>2.216</td>\n",
              "      <td>1.004000</td>\n",
              "      <td>2.480</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.234</td>\n",
              "      <td>1.670</td>\n",
              "      <td>3.132</td>\n",
              "      <td>2.467</td>\n",
              "      <td>2.424</td>\n",
              "      <td>2.345</td>\n",
              "      <td>2.914</td>\n",
              "      <td>2.461</td>\n",
              "      <td>2.129</td>\n",
              "      <td>2.519</td>\n",
              "      <td>1.606</td>\n",
              "      <td>2.363</td>\n",
              "      <td>2.864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>26</td>\n",
              "      <td>3644</td>\n",
              "      <td>1926</td>\n",
              "      <td>4376</td>\n",
              "      <td>2723</td>\n",
              "      <td>4806</td>\n",
              "      <td>1756</td>\n",
              "      <td>10159</td>\n",
              "      <td>2972</td>\n",
              "      <td>13170</td>\n",
              "      <td>10227</td>\n",
              "      <td>1754</td>\n",
              "      <td>14615</td>\n",
              "      <td>18758</td>\n",
              "      <td>12629</td>\n",
              "      <td>9276</td>\n",
              "      <td>10399</td>\n",
              "      <td>2057</td>\n",
              "      <td>842</td>\n",
              "      <td>6454.726316</td>\n",
              "      <td>7175</td>\n",
              "      <td>5860</td>\n",
              "      <td>2154</td>\n",
              "      <td>2159</td>\n",
              "      <td>5930</td>\n",
              "      <td>2979</td>\n",
              "      <td>1770.0</td>\n",
              "      <td>761</td>\n",
              "      <td>10683</td>\n",
              "      <td>10900</td>\n",
              "      <td>11519</td>\n",
              "      <td>2544</td>\n",
              "      <td>10561</td>\n",
              "      <td>7039</td>\n",
              "      <td>6141</td>\n",
              "      <td>4148</td>\n",
              "      <td>9033</td>\n",
              "      <td>...</td>\n",
              "      <td>2.719</td>\n",
              "      <td>2.031</td>\n",
              "      <td>2.429</td>\n",
              "      <td>2.204</td>\n",
              "      <td>2.079</td>\n",
              "      <td>2.852</td>\n",
              "      <td>2.367</td>\n",
              "      <td>2.887</td>\n",
              "      <td>2.543</td>\n",
              "      <td>2.395</td>\n",
              "      <td>2.431</td>\n",
              "      <td>1.712</td>\n",
              "      <td>2.034</td>\n",
              "      <td>2.602</td>\n",
              "      <td>2.598</td>\n",
              "      <td>2.246</td>\n",
              "      <td>2.401</td>\n",
              "      <td>2.142</td>\n",
              "      <td>2.574</td>\n",
              "      <td>2.169</td>\n",
              "      <td>2.877</td>\n",
              "      <td>2.602</td>\n",
              "      <td>3.375</td>\n",
              "      <td>2.382</td>\n",
              "      <td>1.043959</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.543</td>\n",
              "      <td>2.348</td>\n",
              "      <td>1.948</td>\n",
              "      <td>3.782</td>\n",
              "      <td>2.440</td>\n",
              "      <td>2.843</td>\n",
              "      <td>2.499</td>\n",
              "      <td>2.695</td>\n",
              "      <td>2.457</td>\n",
              "      <td>2.254</td>\n",
              "      <td>2.269</td>\n",
              "      <td>1.868</td>\n",
              "      <td>2.142</td>\n",
              "      <td>2.679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>16.33</td>\n",
              "      <td>29.33</td>\n",
              "      <td>25</td>\n",
              "      <td>3096</td>\n",
              "      <td>2191</td>\n",
              "      <td>3784</td>\n",
              "      <td>2344</td>\n",
              "      <td>2675</td>\n",
              "      <td>2063</td>\n",
              "      <td>8561</td>\n",
              "      <td>3208</td>\n",
              "      <td>14310</td>\n",
              "      <td>7538</td>\n",
              "      <td>1571</td>\n",
              "      <td>14405</td>\n",
              "      <td>20381</td>\n",
              "      <td>11549</td>\n",
              "      <td>9488</td>\n",
              "      <td>8848</td>\n",
              "      <td>2175</td>\n",
              "      <td>1109</td>\n",
              "      <td>6150.000000</td>\n",
              "      <td>6085</td>\n",
              "      <td>5977</td>\n",
              "      <td>1828</td>\n",
              "      <td>1392</td>\n",
              "      <td>5996</td>\n",
              "      <td>3021</td>\n",
              "      <td>1562.0</td>\n",
              "      <td>489</td>\n",
              "      <td>8597</td>\n",
              "      <td>6703</td>\n",
              "      <td>8310</td>\n",
              "      <td>2281</td>\n",
              "      <td>11219</td>\n",
              "      <td>7499</td>\n",
              "      <td>7580</td>\n",
              "      <td>4451</td>\n",
              "      <td>7417</td>\n",
              "      <td>...</td>\n",
              "      <td>2.253</td>\n",
              "      <td>1.891</td>\n",
              "      <td>2.540</td>\n",
              "      <td>1.879</td>\n",
              "      <td>2.279</td>\n",
              "      <td>2.548</td>\n",
              "      <td>2.298</td>\n",
              "      <td>2.861</td>\n",
              "      <td>2.528</td>\n",
              "      <td>2.827</td>\n",
              "      <td>2.234</td>\n",
              "      <td>1.404</td>\n",
              "      <td>1.940</td>\n",
              "      <td>2.285</td>\n",
              "      <td>2.436</td>\n",
              "      <td>1.800</td>\n",
              "      <td>2.940</td>\n",
              "      <td>2.152</td>\n",
              "      <td>2.692</td>\n",
              "      <td>1.756</td>\n",
              "      <td>2.607</td>\n",
              "      <td>2.221</td>\n",
              "      <td>3.703</td>\n",
              "      <td>2.175</td>\n",
              "      <td>1.064000</td>\n",
              "      <td>2.058</td>\n",
              "      <td>2.168</td>\n",
              "      <td>2.414</td>\n",
              "      <td>1.885</td>\n",
              "      <td>3.303</td>\n",
              "      <td>2.702</td>\n",
              "      <td>2.617</td>\n",
              "      <td>2.108</td>\n",
              "      <td>2.526</td>\n",
              "      <td>2.151</td>\n",
              "      <td>2.003</td>\n",
              "      <td>2.534</td>\n",
              "      <td>1.996</td>\n",
              "      <td>2.163</td>\n",
              "      <td>2.643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>6.00</td>\n",
              "      <td>9.00</td>\n",
              "      <td>29</td>\n",
              "      <td>3857</td>\n",
              "      <td>2660</td>\n",
              "      <td>3382</td>\n",
              "      <td>2751</td>\n",
              "      <td>3783</td>\n",
              "      <td>2202</td>\n",
              "      <td>9074</td>\n",
              "      <td>2504</td>\n",
              "      <td>11995</td>\n",
              "      <td>10484</td>\n",
              "      <td>1986</td>\n",
              "      <td>13690</td>\n",
              "      <td>20896</td>\n",
              "      <td>15095</td>\n",
              "      <td>10463</td>\n",
              "      <td>11313</td>\n",
              "      <td>2787</td>\n",
              "      <td>935</td>\n",
              "      <td>6454.726316</td>\n",
              "      <td>7009</td>\n",
              "      <td>7279</td>\n",
              "      <td>1719</td>\n",
              "      <td>2238</td>\n",
              "      <td>5620</td>\n",
              "      <td>2816</td>\n",
              "      <td>1934.0</td>\n",
              "      <td>925</td>\n",
              "      <td>11194</td>\n",
              "      <td>11349</td>\n",
              "      <td>10076</td>\n",
              "      <td>2299</td>\n",
              "      <td>11054</td>\n",
              "      <td>7071</td>\n",
              "      <td>6329</td>\n",
              "      <td>5498</td>\n",
              "      <td>8785</td>\n",
              "      <td>...</td>\n",
              "      <td>2.643</td>\n",
              "      <td>2.158</td>\n",
              "      <td>2.489</td>\n",
              "      <td>1.952</td>\n",
              "      <td>2.289</td>\n",
              "      <td>2.767</td>\n",
              "      <td>2.199</td>\n",
              "      <td>3.184</td>\n",
              "      <td>2.388</td>\n",
              "      <td>2.671</td>\n",
              "      <td>2.179</td>\n",
              "      <td>1.536</td>\n",
              "      <td>2.137</td>\n",
              "      <td>2.539</td>\n",
              "      <td>2.481</td>\n",
              "      <td>2.376</td>\n",
              "      <td>2.736</td>\n",
              "      <td>2.163</td>\n",
              "      <td>2.638</td>\n",
              "      <td>2.185</td>\n",
              "      <td>2.709</td>\n",
              "      <td>2.328</td>\n",
              "      <td>3.924</td>\n",
              "      <td>2.529</td>\n",
              "      <td>1.043959</td>\n",
              "      <td>2.663</td>\n",
              "      <td>2.990</td>\n",
              "      <td>2.469</td>\n",
              "      <td>1.922</td>\n",
              "      <td>4.232</td>\n",
              "      <td>2.546</td>\n",
              "      <td>2.937</td>\n",
              "      <td>2.514</td>\n",
              "      <td>2.875</td>\n",
              "      <td>2.837</td>\n",
              "      <td>2.379</td>\n",
              "      <td>2.315</td>\n",
              "      <td>1.954</td>\n",
              "      <td>2.326</td>\n",
              "      <td>2.989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>28</td>\n",
              "      <td>3411</td>\n",
              "      <td>1578</td>\n",
              "      <td>2664</td>\n",
              "      <td>2433</td>\n",
              "      <td>3448</td>\n",
              "      <td>1550</td>\n",
              "      <td>7128</td>\n",
              "      <td>2500</td>\n",
              "      <td>9801</td>\n",
              "      <td>7732</td>\n",
              "      <td>1451</td>\n",
              "      <td>13050</td>\n",
              "      <td>17785</td>\n",
              "      <td>10460</td>\n",
              "      <td>8407</td>\n",
              "      <td>8573</td>\n",
              "      <td>1952</td>\n",
              "      <td>874</td>\n",
              "      <td>6454.726316</td>\n",
              "      <td>5275</td>\n",
              "      <td>6034</td>\n",
              "      <td>2070</td>\n",
              "      <td>1405</td>\n",
              "      <td>3591</td>\n",
              "      <td>2244</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>652</td>\n",
              "      <td>9601</td>\n",
              "      <td>9964</td>\n",
              "      <td>8338</td>\n",
              "      <td>2059</td>\n",
              "      <td>8735</td>\n",
              "      <td>5546</td>\n",
              "      <td>5859</td>\n",
              "      <td>4480</td>\n",
              "      <td>8762</td>\n",
              "      <td>...</td>\n",
              "      <td>2.644</td>\n",
              "      <td>2.128</td>\n",
              "      <td>2.702</td>\n",
              "      <td>2.065</td>\n",
              "      <td>2.597</td>\n",
              "      <td>2.874</td>\n",
              "      <td>2.281</td>\n",
              "      <td>2.128</td>\n",
              "      <td>2.689</td>\n",
              "      <td>2.955</td>\n",
              "      <td>2.370</td>\n",
              "      <td>1.536</td>\n",
              "      <td>1.990</td>\n",
              "      <td>2.545</td>\n",
              "      <td>2.519</td>\n",
              "      <td>2.286</td>\n",
              "      <td>2.839</td>\n",
              "      <td>2.495</td>\n",
              "      <td>2.812</td>\n",
              "      <td>2.104</td>\n",
              "      <td>2.652</td>\n",
              "      <td>2.549</td>\n",
              "      <td>3.105</td>\n",
              "      <td>2.390</td>\n",
              "      <td>1.043959</td>\n",
              "      <td>2.483</td>\n",
              "      <td>2.381</td>\n",
              "      <td>2.498</td>\n",
              "      <td>1.981</td>\n",
              "      <td>3.331</td>\n",
              "      <td>2.954</td>\n",
              "      <td>2.825</td>\n",
              "      <td>2.440</td>\n",
              "      <td>2.848</td>\n",
              "      <td>2.447</td>\n",
              "      <td>2.256</td>\n",
              "      <td>2.497</td>\n",
              "      <td>2.052</td>\n",
              "      <td>2.498</td>\n",
              "      <td>2.829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>1</td>\n",
              "      <td>16.00</td>\n",
              "      <td>27.00</td>\n",
              "      <td>28</td>\n",
              "      <td>3902</td>\n",
              "      <td>2108</td>\n",
              "      <td>4206</td>\n",
              "      <td>2151</td>\n",
              "      <td>3877</td>\n",
              "      <td>2426</td>\n",
              "      <td>7699</td>\n",
              "      <td>3485</td>\n",
              "      <td>11789</td>\n",
              "      <td>11747</td>\n",
              "      <td>2924</td>\n",
              "      <td>15620</td>\n",
              "      <td>19979</td>\n",
              "      <td>14960</td>\n",
              "      <td>9522</td>\n",
              "      <td>9381</td>\n",
              "      <td>1601</td>\n",
              "      <td>844</td>\n",
              "      <td>6454.726316</td>\n",
              "      <td>6546</td>\n",
              "      <td>7567</td>\n",
              "      <td>1978</td>\n",
              "      <td>1405</td>\n",
              "      <td>6207</td>\n",
              "      <td>3621</td>\n",
              "      <td>1393.0</td>\n",
              "      <td>792</td>\n",
              "      <td>8728</td>\n",
              "      <td>11456</td>\n",
              "      <td>9916</td>\n",
              "      <td>2942</td>\n",
              "      <td>12863</td>\n",
              "      <td>7089</td>\n",
              "      <td>6221</td>\n",
              "      <td>5619</td>\n",
              "      <td>10623</td>\n",
              "      <td>...</td>\n",
              "      <td>2.320</td>\n",
              "      <td>2.244</td>\n",
              "      <td>2.487</td>\n",
              "      <td>1.779</td>\n",
              "      <td>2.361</td>\n",
              "      <td>2.548</td>\n",
              "      <td>2.003</td>\n",
              "      <td>2.541</td>\n",
              "      <td>2.469</td>\n",
              "      <td>2.482</td>\n",
              "      <td>2.259</td>\n",
              "      <td>1.623</td>\n",
              "      <td>1.947</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.308</td>\n",
              "      <td>2.654</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.270</td>\n",
              "      <td>2.638</td>\n",
              "      <td>2.265</td>\n",
              "      <td>2.311</td>\n",
              "      <td>2.441</td>\n",
              "      <td>3.410</td>\n",
              "      <td>2.226</td>\n",
              "      <td>1.043959</td>\n",
              "      <td>2.421</td>\n",
              "      <td>2.452</td>\n",
              "      <td>2.482</td>\n",
              "      <td>1.851</td>\n",
              "      <td>2.286</td>\n",
              "      <td>2.455</td>\n",
              "      <td>2.473</td>\n",
              "      <td>2.538</td>\n",
              "      <td>2.726</td>\n",
              "      <td>2.092</td>\n",
              "      <td>2.342</td>\n",
              "      <td>2.461</td>\n",
              "      <td>1.837</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1603</th>\n",
              "      <td>2</td>\n",
              "      <td>23.00</td>\n",
              "      <td>36.00</td>\n",
              "      <td>24</td>\n",
              "      <td>2713</td>\n",
              "      <td>1694</td>\n",
              "      <td>3153</td>\n",
              "      <td>1766</td>\n",
              "      <td>2685</td>\n",
              "      <td>2081</td>\n",
              "      <td>9003</td>\n",
              "      <td>2927</td>\n",
              "      <td>11843</td>\n",
              "      <td>9890</td>\n",
              "      <td>2308</td>\n",
              "      <td>12994</td>\n",
              "      <td>17177</td>\n",
              "      <td>11485</td>\n",
              "      <td>10095</td>\n",
              "      <td>7990</td>\n",
              "      <td>1763</td>\n",
              "      <td>798</td>\n",
              "      <td>6454.726316</td>\n",
              "      <td>6614</td>\n",
              "      <td>6391</td>\n",
              "      <td>1890</td>\n",
              "      <td>1683</td>\n",
              "      <td>5660</td>\n",
              "      <td>2538</td>\n",
              "      <td>1500.0</td>\n",
              "      <td>842</td>\n",
              "      <td>7753</td>\n",
              "      <td>8109</td>\n",
              "      <td>8618</td>\n",
              "      <td>2172</td>\n",
              "      <td>7058</td>\n",
              "      <td>6358</td>\n",
              "      <td>4903</td>\n",
              "      <td>5022</td>\n",
              "      <td>9294</td>\n",
              "      <td>...</td>\n",
              "      <td>2.187</td>\n",
              "      <td>1.522</td>\n",
              "      <td>2.373</td>\n",
              "      <td>1.599</td>\n",
              "      <td>2.422</td>\n",
              "      <td>2.468</td>\n",
              "      <td>2.347</td>\n",
              "      <td>2.838</td>\n",
              "      <td>2.221</td>\n",
              "      <td>2.426</td>\n",
              "      <td>2.058</td>\n",
              "      <td>1.505</td>\n",
              "      <td>1.662</td>\n",
              "      <td>2.792</td>\n",
              "      <td>2.121</td>\n",
              "      <td>2.253</td>\n",
              "      <td>2.750</td>\n",
              "      <td>1.976</td>\n",
              "      <td>2.314</td>\n",
              "      <td>1.767</td>\n",
              "      <td>2.364</td>\n",
              "      <td>2.052</td>\n",
              "      <td>3.201</td>\n",
              "      <td>2.170</td>\n",
              "      <td>1.043959</td>\n",
              "      <td>2.457</td>\n",
              "      <td>2.690</td>\n",
              "      <td>2.273</td>\n",
              "      <td>1.693</td>\n",
              "      <td>2.943</td>\n",
              "      <td>2.711</td>\n",
              "      <td>2.375</td>\n",
              "      <td>2.182</td>\n",
              "      <td>2.533</td>\n",
              "      <td>2.317</td>\n",
              "      <td>1.808</td>\n",
              "      <td>2.282</td>\n",
              "      <td>1.748</td>\n",
              "      <td>2.430</td>\n",
              "      <td>2.701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1604</th>\n",
              "      <td>2</td>\n",
              "      <td>28.00</td>\n",
              "      <td>40.00</td>\n",
              "      <td>23</td>\n",
              "      <td>3123</td>\n",
              "      <td>1630</td>\n",
              "      <td>4345</td>\n",
              "      <td>1937</td>\n",
              "      <td>3946</td>\n",
              "      <td>1767</td>\n",
              "      <td>8170</td>\n",
              "      <td>3380</td>\n",
              "      <td>13104</td>\n",
              "      <td>8393</td>\n",
              "      <td>2021</td>\n",
              "      <td>16168</td>\n",
              "      <td>18640</td>\n",
              "      <td>12868</td>\n",
              "      <td>11232</td>\n",
              "      <td>9629</td>\n",
              "      <td>2217</td>\n",
              "      <td>840</td>\n",
              "      <td>6454.726316</td>\n",
              "      <td>8379</td>\n",
              "      <td>8229</td>\n",
              "      <td>2714</td>\n",
              "      <td>1594</td>\n",
              "      <td>7492</td>\n",
              "      <td>2933</td>\n",
              "      <td>1453.0</td>\n",
              "      <td>668</td>\n",
              "      <td>10203</td>\n",
              "      <td>9111</td>\n",
              "      <td>10515</td>\n",
              "      <td>2090</td>\n",
              "      <td>12227</td>\n",
              "      <td>7888</td>\n",
              "      <td>6518</td>\n",
              "      <td>5305</td>\n",
              "      <td>9113</td>\n",
              "      <td>...</td>\n",
              "      <td>1.975</td>\n",
              "      <td>2.242</td>\n",
              "      <td>2.482</td>\n",
              "      <td>1.892</td>\n",
              "      <td>2.155</td>\n",
              "      <td>2.635</td>\n",
              "      <td>2.378</td>\n",
              "      <td>2.424</td>\n",
              "      <td>2.407</td>\n",
              "      <td>2.298</td>\n",
              "      <td>2.391</td>\n",
              "      <td>1.481</td>\n",
              "      <td>1.918</td>\n",
              "      <td>2.322</td>\n",
              "      <td>2.524</td>\n",
              "      <td>1.964</td>\n",
              "      <td>2.990</td>\n",
              "      <td>2.176</td>\n",
              "      <td>2.636</td>\n",
              "      <td>1.955</td>\n",
              "      <td>2.445</td>\n",
              "      <td>2.309</td>\n",
              "      <td>3.634</td>\n",
              "      <td>1.931</td>\n",
              "      <td>1.043959</td>\n",
              "      <td>2.581</td>\n",
              "      <td>2.374</td>\n",
              "      <td>2.228</td>\n",
              "      <td>1.670</td>\n",
              "      <td>2.462</td>\n",
              "      <td>2.628</td>\n",
              "      <td>2.551</td>\n",
              "      <td>2.169</td>\n",
              "      <td>2.873</td>\n",
              "      <td>2.063</td>\n",
              "      <td>2.170</td>\n",
              "      <td>2.732</td>\n",
              "      <td>1.894</td>\n",
              "      <td>2.291</td>\n",
              "      <td>2.803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1605</th>\n",
              "      <td>2</td>\n",
              "      <td>18.33</td>\n",
              "      <td>29.33</td>\n",
              "      <td>23</td>\n",
              "      <td>2411</td>\n",
              "      <td>1749</td>\n",
              "      <td>2297</td>\n",
              "      <td>1800</td>\n",
              "      <td>2979</td>\n",
              "      <td>1901</td>\n",
              "      <td>5947</td>\n",
              "      <td>2934</td>\n",
              "      <td>7588</td>\n",
              "      <td>7609</td>\n",
              "      <td>1662</td>\n",
              "      <td>11397</td>\n",
              "      <td>15116</td>\n",
              "      <td>9759</td>\n",
              "      <td>6644</td>\n",
              "      <td>6461</td>\n",
              "      <td>1663</td>\n",
              "      <td>469</td>\n",
              "      <td>5231.000000</td>\n",
              "      <td>5289</td>\n",
              "      <td>5055</td>\n",
              "      <td>1204</td>\n",
              "      <td>1098</td>\n",
              "      <td>4157</td>\n",
              "      <td>1528</td>\n",
              "      <td>1209.0</td>\n",
              "      <td>376</td>\n",
              "      <td>9532</td>\n",
              "      <td>6771</td>\n",
              "      <td>8745</td>\n",
              "      <td>2212</td>\n",
              "      <td>7365</td>\n",
              "      <td>5692</td>\n",
              "      <td>5525</td>\n",
              "      <td>3017</td>\n",
              "      <td>7082</td>\n",
              "      <td>...</td>\n",
              "      <td>2.520</td>\n",
              "      <td>1.648</td>\n",
              "      <td>2.414</td>\n",
              "      <td>1.736</td>\n",
              "      <td>1.962</td>\n",
              "      <td>2.296</td>\n",
              "      <td>1.784</td>\n",
              "      <td>2.468</td>\n",
              "      <td>1.769</td>\n",
              "      <td>2.079</td>\n",
              "      <td>1.926</td>\n",
              "      <td>1.232</td>\n",
              "      <td>1.501</td>\n",
              "      <td>2.476</td>\n",
              "      <td>1.831</td>\n",
              "      <td>1.939</td>\n",
              "      <td>3.250</td>\n",
              "      <td>1.900</td>\n",
              "      <td>2.403</td>\n",
              "      <td>1.680</td>\n",
              "      <td>2.193</td>\n",
              "      <td>1.968</td>\n",
              "      <td>3.351</td>\n",
              "      <td>1.494</td>\n",
              "      <td>0.997000</td>\n",
              "      <td>1.828</td>\n",
              "      <td>3.025</td>\n",
              "      <td>1.852</td>\n",
              "      <td>1.463</td>\n",
              "      <td>2.982</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.408</td>\n",
              "      <td>1.849</td>\n",
              "      <td>2.449</td>\n",
              "      <td>2.574</td>\n",
              "      <td>1.784</td>\n",
              "      <td>2.515</td>\n",
              "      <td>1.806</td>\n",
              "      <td>2.071</td>\n",
              "      <td>2.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1606</th>\n",
              "      <td>1</td>\n",
              "      <td>8.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>29</td>\n",
              "      <td>2933</td>\n",
              "      <td>2058</td>\n",
              "      <td>4653</td>\n",
              "      <td>2922</td>\n",
              "      <td>4766</td>\n",
              "      <td>2883</td>\n",
              "      <td>8638</td>\n",
              "      <td>2867</td>\n",
              "      <td>13211</td>\n",
              "      <td>9493</td>\n",
              "      <td>2706</td>\n",
              "      <td>12646</td>\n",
              "      <td>17313</td>\n",
              "      <td>13565</td>\n",
              "      <td>11755</td>\n",
              "      <td>11343</td>\n",
              "      <td>2467</td>\n",
              "      <td>733</td>\n",
              "      <td>6454.726316</td>\n",
              "      <td>7443</td>\n",
              "      <td>7815</td>\n",
              "      <td>2089</td>\n",
              "      <td>1233</td>\n",
              "      <td>5908</td>\n",
              "      <td>3503</td>\n",
              "      <td>1754.0</td>\n",
              "      <td>683</td>\n",
              "      <td>8161</td>\n",
              "      <td>10624</td>\n",
              "      <td>11091</td>\n",
              "      <td>2986</td>\n",
              "      <td>11887</td>\n",
              "      <td>7461</td>\n",
              "      <td>6462</td>\n",
              "      <td>4448</td>\n",
              "      <td>10473</td>\n",
              "      <td>...</td>\n",
              "      <td>2.257</td>\n",
              "      <td>2.148</td>\n",
              "      <td>2.453</td>\n",
              "      <td>1.965</td>\n",
              "      <td>2.315</td>\n",
              "      <td>2.748</td>\n",
              "      <td>2.459</td>\n",
              "      <td>2.506</td>\n",
              "      <td>2.481</td>\n",
              "      <td>2.498</td>\n",
              "      <td>2.325</td>\n",
              "      <td>1.758</td>\n",
              "      <td>2.014</td>\n",
              "      <td>2.160</td>\n",
              "      <td>2.673</td>\n",
              "      <td>2.217</td>\n",
              "      <td>2.889</td>\n",
              "      <td>2.228</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.152</td>\n",
              "      <td>2.700</td>\n",
              "      <td>2.486</td>\n",
              "      <td>3.799</td>\n",
              "      <td>2.542</td>\n",
              "      <td>1.043959</td>\n",
              "      <td>2.598</td>\n",
              "      <td>2.895</td>\n",
              "      <td>2.518</td>\n",
              "      <td>2.002</td>\n",
              "      <td>3.910</td>\n",
              "      <td>2.442</td>\n",
              "      <td>2.778</td>\n",
              "      <td>2.445</td>\n",
              "      <td>2.769</td>\n",
              "      <td>2.480</td>\n",
              "      <td>2.198</td>\n",
              "      <td>2.514</td>\n",
              "      <td>2.047</td>\n",
              "      <td>2.456</td>\n",
              "      <td>2.795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1607 rows  144 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      DX_bl  ADAS11  ADAS13  MMSE  ...  ST95TA  ST97TA  ST98TA  ST99TA\n",
              "0         0    9.00   14.00    27  ...   2.519   1.606   2.363   2.864\n",
              "1         0    5.00    7.00    26  ...   2.269   1.868   2.142   2.679\n",
              "2         2   16.33   29.33    25  ...   2.534   1.996   2.163   2.643\n",
              "3         1    6.00    9.00    29  ...   2.315   1.954   2.326   2.989\n",
              "4         0    5.00    8.00    28  ...   2.497   2.052   2.498   2.829\n",
              "...     ...     ...     ...   ...  ...     ...     ...     ...     ...\n",
              "1602      1   16.00   27.00    28  ...   2.461   1.837   2.305   2.716\n",
              "1603      2   23.00   36.00    24  ...   2.282   1.748   2.430   2.701\n",
              "1604      2   28.00   40.00    23  ...   2.732   1.894   2.291   2.803\n",
              "1605      2   18.33   29.33    23  ...   2.515   1.806   2.071   2.248\n",
              "1606      1    8.00   15.00    29  ...   2.514   2.047   2.456   2.795\n",
              "\n",
              "[1607 rows x 144 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrwCHaNJ-9KS"
      },
      "source": [
        "##**Define 10 Fold Cross Validate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwGYH5STTFVR"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqW-_umVRVsr"
      },
      "source": [
        "#**Task 1 (3class Classification)**\n",
        "##**::Classifier(X, y)::**\n",
        "This Class is consist of various models to classify the 'DX_bl', which labels are 0 (Cognitive Normal), 1 (Mild Cogitive Impariment), 2 (Alzheimer's Disease).\n",
        ">\n",
        "Each model can be selected whether the model should be trained and tested with the whole data, or **10-fold cross valdation**. Each of them also has a method for predicting.\n",
        ">\n",
        "The Class has total 11 models for classifying, and each of them can be called by \n",
        "\n",
        ">```classifiers = Classifier(X, y)```\n",
        "```classifiers.SgdClassifier(cross=True, predict=X[0])```\n",
        "\n",
        "###**Model Parameters**###\n",
        "```classifiers.model(cross = True, predict = None)```\n",
        "####**cross = True(default)**\n",
        "> This parameter decide whether the model train and test with the 10 fold cross validation or just raw set.\n",
        "\n",
        "####**predict = None(default)**\n",
        "> With this parameter, if predict parameter exists, model return the predicted value of the model.\n",
        "\n",
        "When the parameter ***cross*** is set to True, the model returns follow 3 items.\n",
        "\n",
        "1. The **mean** of the test scores by 10 fold cross validation.\n",
        "2. The array of the ten test scores by 10 fold cross validation.\n",
        "\n",
        "When the parameter ***cross*** is set to False, the model returns following item.\n",
        "\n",
        "1. The test score by train dataset.\n",
        "\n",
        "When the parameter ***predict*** is set to True, the model returns\n",
        "1. if **cross** = True, => The array of the ten predicted value by 10 fold cross validation.\n",
        "2. if **cross** = False, => The predicted value from the model.\n",
        "---\n",
        "###**Models**\n",
        "\n",
        "> **Logistic Regression Classifier**: Classifier using Logistic Regression. (max_iter=7600, C=1e)\n",
        "```classifiers.logiClassifier(cross=True)```\n",
        "\n",
        "> **Decision Tree Classifier**: Classifier using DecisionTree.\n",
        "```classifiers.DeciTreeClassifier(cross=True)```\n",
        "\n",
        "> **Support Vector Machine Classifier**: Classifier using SVM. Preprocessed with the StandardScaler at scikit learn\n",
        "```classifiers.SVMClassifier(cross=True)```\n",
        "\n",
        "> **KNeighbor Classifier**: Classifier using K Neighbor. K is set to 40.\n",
        "```classifiers.KNeighClassifier(cross=True)```\n",
        "\n",
        "> **Random Forest Classifier**: Classifier using Random Forest. max depth is set to 40.\n",
        "```classifiers.RanForestClassifier(cross=True)```\n",
        "\n",
        "> **Stochastic Gradient Descent**: Classifier using Stochastic Gradient Descent.\n",
        "```classifiers.SGDClassifier(cross=True)```\n",
        "\n",
        "> **Neural Network Classifier**: Classifier using Neural Network. Hidden layer is set to (5,2).\n",
        "```classifiers.NeuClassifier(cross=True)```\n",
        "\n",
        "> **Linear Support Vector Classifier**: Classifier using Linear Support Vector. Preprocessed with the StandardScaler at scikit learn.\n",
        "```classifiers.LVCClassifier(cross=True)```\n",
        "\n",
        "> **Perceptron Classifier**: Classifier using Perceptron.\n",
        "```classifiers.PerceptronClassifier(cross=True)```\n",
        "\n",
        "> **Polynomial Kernel SVM Classifier**: Classifier using SVM with Poly Kernel. Preprocessed with the StandardScaler at scikit learn.\n",
        "```classifiers.PolyClassifier(cross=True)```\n",
        "\n",
        "> **Gaussian Rbf Kernel Classifier**: Classifier using Gaussian Rbf Kernel. Preprocessed with the StandardScaler at scikit learn.\n",
        "```classifiers.RbfClassifier(cross=True)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMNICqz8Trpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9913b7-1f1b-4bec-8a85-3638babb5c6d"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "#Get the data of 140 Columns (including CV, AT)\n",
        "X = Data.getVolumeArray('All')\n",
        "\n",
        "#Get the diagnosis Column\n",
        "y = Data.getDiagnosisArray()\n",
        "\n",
        "class Classifier:\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "  \n",
        "  def common(self, classifier, cross = True, predict = None):\n",
        "    if cross is True:\n",
        "      predictions = cross_val_predict(classifier, X, y, cv=kfold)\n",
        "      crossValidationResults = cross_validate(classifier, X, y, cv=kfold, return_train_score=True)\n",
        "      sum = 0\n",
        "      for index in range(10):\n",
        "        sum = sum + crossValidationResults['test_score'][index]\n",
        "      mean = sum / 10\n",
        "\n",
        "      if predict is not None:\n",
        "        print('mean of cross validation test score:',mean, end=\"\\n\\n\")\n",
        "        print('cross validation test score:',crossValidationResults['test_score'], end=\"\\n\\n\")\n",
        "        print('predictions:',predictions, end=\"\\n\\n\")\n",
        "        print('return values ----------', end=\"\\n\\n\")\n",
        "        return mean, crossValidationResults['test_score'], predictions\n",
        "      else:\n",
        "        print('mean of cross validation test score:',mean)\n",
        "        print('cross validation test score:',crossValidationResults['test_score'], end=\"\\n\\n\")\n",
        "        print('return values ----------', end=\"\\n\\n\")\n",
        "        return mean, crossValidationResults['test_score']\n",
        "\n",
        "    elif cross is False:\n",
        "      classifier.fit(self.X, self.y)\n",
        "      prediction = classifier.predict([predict])\n",
        "      if predict is not None:\n",
        "        print('score of full data:',classifier.score(self.X, self.y))\n",
        "        print('prediction:', prediction, end=\"\\n\\n\")\n",
        "        print('return values ----------', end=\"\\n\\n\")\n",
        "        return classifier.score(self.X, self.y), prediction\n",
        "      else:\n",
        "        print('score of full data:',classifier.score(self.X, self.y), end=\"\\n\\n\")\n",
        "        print('return values ----------', end=\"\\n\\n\")\n",
        "        return classifier.score(self.X, self.y)\n",
        "\n",
        "  #Logisitic Regression Classifier\n",
        "  def LogClassifier(self, cross, predict= None):\n",
        "    #c = 1e5 0.5108928571428571 #default 0.5102717391304348 #1e3 = 0.5102717391304348 #1e10 = 0.5108928571428571\n",
        "    log = LogisticRegression(max_iter=7600, C=1e5)\n",
        "    return self.common(log, cross, predict)\n",
        "\n",
        "  #Decision Tree Classifier\n",
        "  def DeciTreeClassifier(self, cross, predict= None):\n",
        "    #0.4617546583850931\n",
        "    dtc = DecisionTreeClassifier(random_state=0)\n",
        "    return self.common(dtc, cross, predict)\n",
        "\n",
        "  #Support Vector Machine with Linear Kernel\n",
        "  def SVMClassifier(self, cross, predict = None):\n",
        "    #0.547014751552795\n",
        "    svm = make_pipeline(StandardScaler(), SVC(kernel='linear', degree=2, gamma='auto'))\n",
        "    return self.common(svm, cross, predict)\n",
        "\n",
        "  #KNeighbor\n",
        "  def KNeighClassifier(self, cross, predict = None):\n",
        "    #K = 3 => 0.45053959627329193\n",
        "    #K = 40 => 0.4872437888198757\n",
        "    #K = 10 => 0.46856754658385097\n",
        "    #K = 200 => 0.5090217391304347\n",
        "    neigh = KNeighborsClassifier(n_neighbors=40)\n",
        "    return self.common(neigh, cross, predict)\n",
        "\n",
        "  #Random Forest\n",
        "  def RanForestClassifier(self, cross, predict = None):\n",
        "    #0.5152445652173914 at maxdepth = 2\n",
        "    #0.5494836956521738 at maxdepth = 40\n",
        "    rfc = RandomForestClassifier(max_depth=40, random_state=0)\n",
        "    return self.common(rfc, cross, predict)\n",
        "\n",
        "  #Stochastic Gradient Descent\n",
        "  def SgdClassifier(self, cross, predict = None):\n",
        "    #0.4922282608695653\n",
        "    sgd = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000))\n",
        "    return self.common(sgd, cross, predict)\n",
        "\n",
        "  #Neural Network\n",
        "  def NeuClassifier(self, cross, predict = None):\n",
        "    #0.5065062111801243\n",
        "    neu = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
        "    return self.common(neu, cross, predict)\n",
        "\n",
        "  #Linear Support Vector Classifier <=> SVM with linear kernel\n",
        "  def LinSVCClassifier(self, cross, predict = None):\n",
        "    #0.5028454968944099\n",
        "    linsvc = make_pipeline(StandardScaler(), LinearSVC(tol=1e-5, dual=False))\n",
        "    return self.common(linsvc, cross, predict)\n",
        "\n",
        "  #Perceptron Classifier\n",
        "  def PerceptronClassifier(self, cross, predict = None):\n",
        "    #0.4647399068322981\n",
        "    perc = Perceptron(tol=1e-3, random_state=0)\n",
        "    return self.common(perc, cross, predict)\n",
        "\n",
        "  #Polynomial Kernel SVM Classifier\n",
        "  def PolyClassifier(self, cross, predict = None):\n",
        "    #0.5221467391304349\n",
        "    poly_kernel_svm_clf = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('svm_clf', SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "    ])\n",
        "    return self.common(poly_kernel_svm_clf, cross, predict)\n",
        "\n",
        "  #Gaussian RBF Kernel SVM Classifier\n",
        "  def RbfClassifier(self, cross, predict = None):\n",
        "    #0.5065062111801243\n",
        "    rbf_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=\"auto\", C=0.001))\n",
        "    ])\n",
        "    return self.common(rbf_kernel_svm_clf, cross, predict)\n",
        "\n",
        "classifiers = Classifier(X, y)\n",
        "\n",
        "classifiers.LinSVCClassifier(cross=True, predict=X[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean of cross validation test score: 0.5028454968944099\n",
            "\n",
            "cross validation test score: [0.59006211 0.43478261 0.43478261 0.47204969 0.47826087 0.49689441\n",
            " 0.54037267 0.5625     0.50625    0.5125    ]\n",
            "\n",
            "predictions: [1 0 1 ... 1 1 1]\n",
            "\n",
            "return values ----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5028454968944099,\n",
              " array([0.59006211, 0.43478261, 0.43478261, 0.47204969, 0.47826087,\n",
              "        0.49689441, 0.54037267, 0.5625    , 0.50625   , 0.5125    ]),\n",
              " array([1, 0, 1, ..., 1, 1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMf-_cUZTr-X"
      },
      "source": [
        "##**Model Comparisions & Selection**\n",
        "\n",
        "###**def ClassifierSelector**\n",
        "This function create the dictionary each models' mean of test scores that are derived by 10-Fold Cross Validation.\n",
        "Also it creates the different mean of test scores by the data, **whether the missing features are filled by mean, median, or zero.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC27_o7QE93l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c27d3e7-10b6-4625-9b5c-7cf718a45871"
      },
      "source": [
        "def ClassifierSelector():\n",
        "  dictionary = {}\n",
        "  for datatype in [\"Mean\", \"Zero\", \"Median\"]:\n",
        "    dictionary[datatype] = {}\n",
        "    if datatype == \"Mean\":\n",
        "      Data = FullData(MissingTask(df).Mean())\n",
        "    elif datatype == \"Zero\":\n",
        "      Data = FullData(MissingTask(df).Zero())\n",
        "    elif datatype == \"Median\":\n",
        "      Data = FullData(MissingTask(df).Median())\n",
        "    \n",
        "    X = Data.getVolumeArray('All')\n",
        "    y = Data.getDiagnosisArray()\n",
        "\n",
        "    log = [LogisticRegression(max_iter=7600, C=1e5), 'Logistic']\n",
        "    dtc = [DecisionTreeClassifier(random_state=0), 'Decision Tree']\n",
        "    svm = [make_pipeline(StandardScaler(), SVC(gamma='auto')), 'SVM']\n",
        "    neigh = [KNeighborsClassifier(n_neighbors=40), 'k Neigh']\n",
        "    rfc = [RandomForestClassifier(max_depth=40, random_state=0), 'Random Forest']\n",
        "    sgd = [make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000)), 'SGD']\n",
        "    neu = [MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1), 'Neural Network']\n",
        "    linsvc = [make_pipeline(StandardScaler(), LinearSVC(tol=1e-5, dual=False)), 'Linear SVC']\n",
        "    perc = [Perceptron(tol=1e-3, random_state=0), 'Perceptron']\n",
        "    poly_kernel_svm_clf = [Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('svm_clf', SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "    ]), 'Poly Kernel']\n",
        "    rbf_kernel_svm_clf = [Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "    ]), 'Gaussian RBF Kernel']\n",
        "\n",
        "    models = [log, dtc, svm, neigh, rfc, sgd, neu, linsvc, perc, poly_kernel_svm_clf, rbf_kernel_svm_clf]\n",
        "    for model in models:\n",
        "      sum = 0\n",
        "      crossValidationResults = cross_validate(model[0], X, y, cv=kfold, return_train_score=True)\n",
        "      for index in range(10):\n",
        "        sum = sum + crossValidationResults['test_score'][index]\n",
        "      mean = sum / 10\n",
        "      dictionary[datatype][model[1]] = mean\n",
        "  return dictionary\n",
        "ClassifierSelector()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Mean': {'Decision Tree': 0.4617546583850931,\n",
              "  'Gaussian RBF Kernel': 0.5065062111801243,\n",
              "  'Linear SVC': 0.5028454968944099,\n",
              "  'Logistic': 0.5108928571428571,\n",
              "  'Neural Network': 0.5065062111801243,\n",
              "  'Perceptron': 0.4647399068322981,\n",
              "  'Poly Kernel': 0.5221467391304349,\n",
              "  'Random Forest': 0.5494836956521738,\n",
              "  'SGD': 0.47864130434782604,\n",
              "  'SVM': 0.547014751552795,\n",
              "  'k Neigh': 0.4872437888198757},\n",
              " 'Median': {'Decision Tree': 0.45864130434782613,\n",
              "  'Gaussian RBF Kernel': 0.5065062111801243,\n",
              "  'Linear SVC': 0.5034666149068323,\n",
              "  'Logistic': 0.5096506211180125,\n",
              "  'Neural Network': 0.5065062111801243,\n",
              "  'Perceptron': 0.4268827639751553,\n",
              "  'Poly Kernel': 0.5215256211180125,\n",
              "  'Random Forest': 0.5457453416149068,\n",
              "  'SGD': 0.4816459627329192,\n",
              "  'SVM': 0.547639751552795,\n",
              "  'k Neigh': 0.48661878881987575},\n",
              " 'Zero': {'Decision Tree': 0.45368012422360254,\n",
              "  'Gaussian RBF Kernel': 0.5065062111801243,\n",
              "  'Linear SVC': 0.5040683229813665,\n",
              "  'Logistic': 0.5059549689440994,\n",
              "  'Neural Network': 0.5065062111801243,\n",
              "  'Perceptron': 0.45367624223602476,\n",
              "  'Poly Kernel': 0.5239868012422361,\n",
              "  'Random Forest': 0.5544487577639752,\n",
              "  'SGD': 0.4952989130434783,\n",
              "  'SVM': 0.5420225155279503,\n",
              "  'k Neigh': 0.4841187888198758}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hJZB5fB96p3"
      },
      "source": [
        "##**Comparision of the mean of the test scores by each models that are dervied by each different dataframes (Mean, Median, Zero)**\n",
        "\n",
        "As below, each test scores by 10 fold cross validation at the different data frames which are filled with ZERO, MEAN, MEDIAN.\n",
        "\n",
        "###**Mean**###\n",
        "At the dataframe whose missing values are filled by ***mean***, the **Random forest** has the biggest accuracy of mean of 10 fold cross validation test score. \n",
        "> **Winner: Random Forest (0.5494836956521738)**\n",
        "\n",
        "###**Median**###\n",
        "At the dataframe whose missing values are filled by ***median***, the **Support Vector Machine** has the biggest accuracy of median of 10 fold cross validation test score.\n",
        "> **Winner: Support Vector Machine (0.547639751552795)**\n",
        "\n",
        "###**Zero**###\n",
        "At the dataframe whose missing values are filled by ***zero***, the **Random Forest** has the biggest accuracy of zero of 10 fold cross validation test score.\n",
        "> **Winner: Random Forest (0.5544487577639752)**\n",
        "---\n",
        "##**The DataFrame whose missing values are filled with ZERO has the biggest results of test score**\n",
        "\n",
        "> ***Zero(0.5544487577639752) > Mean(0.5494836956521738) > Median(0.547639751552795)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzJJdywkxrF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228cc3b3-de12-4337-c799-91c62e335246"
      },
      "source": [
        "mean = [0.4617546583850931, 0.5065062111801243, 0.5028454968944099, 0.5108928571428571, 0.5065062111801243, 0.4647399068322981, 0.5221467391304349, 0.5494836956521738, 0.4785481366459628, 0.547014751552795, 0.4872437888198757]\n",
        "#1. Random Forest #2. SVM\n",
        "#0.5494836956521738\n",
        "median = [0.45864130434782613, 0.5065062111801243, 0.5034666149068323, 0.5096506211180125, 0.5065062111801243, 0.4268827639751553, 0.5215256211180125, 0.5457453416149068, 0.4797437888198758,0.547639751552795, 0.48661878881987575]\n",
        "#1. SVM #2. Random Forest\n",
        "#0.547639751552795\n",
        "zero = [0.45368012422360254, 0.5065062111801243, 0.5040683229813665, 0.5059549689440994, 0.5065062111801243, 0.45367624223602476, 0.5239868012422361, 0.5544487577639752, 0.4717041925465838, 0.5420225155279503, 0.4841187888198758]\n",
        "#1. Random Forest #2.SVM\n",
        "#0.5544487577639752\n",
        "\n",
        "#Zero > Mean > Median\n",
        "print('max of mean data:',np.max(mean))\n",
        "print('max of median data:',np.max(median))\n",
        "print('max of zero data:',np.max(zero))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max of mean data: 0.5494836956521738\n",
            "max of median data: 0.547639751552795\n",
            "max of zero data: 0.5544487577639752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSsqTASKTOno"
      },
      "source": [
        "#**Task 2 (3-logit Regression)**\n",
        "##**::Regressor::**\n",
        "This Class is consist of various models to regression the 'ADAS11', 'ADAS13', 'MMSE'.\n",
        ">\n",
        "Each model can be selected whether the model should be trained and tested with the whole data, or **10-fold cross valdation**. Each of them also has a method for predicting.\n",
        ">\n",
        "The Class has total 5 models for regression, and each of them can be called by \n",
        "\n",
        ">```regressors = Regressor(X, y)```\n",
        "```regressors.LinearRegressor(cross=True, predict=X[0], predictIndex = None)```\n",
        "\n",
        "###**Model Parameters**###\n",
        "```regressors.model(cross = True, predict = None, predictIndex = None)```\n",
        "####**cross = True(default)**\n",
        "> This parameter decide whether the model train and test with the 10 fold cross validation or just raw set.\n",
        "\n",
        "####**predict = None(default)**\n",
        "> With this parameter, if predict parameter exists, model return the predicted value of the model.\n",
        "\n",
        "####**predictIndex = None(default)**\n",
        "> This parameter helps to derive the actual value by derving the y[predictIndex]\n",
        "\n",
        "When the parameter ***cross*** is set to True, the model returns follow 3 items.\n",
        "\n",
        "1. The **mean** of the test scores by 10 fold cross validation.\n",
        "2. The array of the ten test scores by 10 fold cross validation.\n",
        "\n",
        "When the parameter ***cross*** is set to False, the model returns following item.\n",
        "\n",
        "1. The test score by train dataset.\n",
        "\n",
        "When the parameter ***predict*** is set to True, the model returns\n",
        "1. if **cross** = True, => \n",
        "  1. The array of the ten predicted values by 10 fold cross validation.\n",
        "  2. The actual values of ADAS11, ADAS13, MMSE\n",
        "2. if **cross** = False, => \n",
        "  1. The predicted values from the model that trained by full data.\n",
        "  2. The actual values of ADAS11, ADAS13, MMSE\n",
        "---\n",
        "###**Models**\n",
        "\n",
        "> **Linear Regression**: Regression using Linear Regression.\n",
        "```regressors.LinearRegressor(cross=True)```\n",
        "\n",
        "> **Elastic Net Regression**: Regression using Elastic Net.\n",
        "```regressors.ElasticRegressor(cross=True)```\n",
        "\n",
        "> **Multi Task Elastic Net Regression**: Regression using Multi Task Elastic Net.\n",
        "```regressors.MultiTaskElasticRegressor(cross= True)```\n",
        "\n",
        "> **Ridge Regression**: Regression using Ridge Regression.\n",
        "```regressors.RidRegressor(cross=True)```\n",
        "\n",
        "> **Lasso Regression**: Linear Model trained with L1 prior as regularizer.(=lasso)\n",
        "```regressors.LasRegressor(cross=True)```\n",
        "\n",
        "> **Multi Task Lasso Regression**: Multi Task Lasso Regresson.\n",
        "```regressors.MultiTaskLassoRegressor(cross=True)```\n",
        "\n",
        "> **KNeighbors Regressionm**: Regression using K Neighbors. K is set to 40.\n",
        "```regressors.KNeighRegressor(cross=True)```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljwzm0t5r5Xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a569f98f-9c75-486c-945a-3cefcc17777e"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn import linear_model\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X = Data.getVolumeArray('All')\n",
        "y = Data.getScoreArray('All')\n",
        "\n",
        "class Regressor:\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def common(self, regressor, cross = False, predict = None, predictIndex = None):\n",
        "    if cross is True:\n",
        "      predictions = cross_val_predict(regressor, X, y, cv=kfold)\n",
        "      crossValidationResults = cross_validate(regressor, X, y, cv=kfold, scoring=('r2'), return_train_score=True)\n",
        "      sum = 0\n",
        "      for index in range(10):\n",
        "        sum = sum + crossValidationResults['test_score'][index]\n",
        "      mean = sum / 10\n",
        "\n",
        "      if predict is not None:\n",
        "        print('mean of cross validation test score:',mean, end=\"\\n\\n\")\n",
        "        print('cross validation test score:',crossValidationResults['test_score'], end=\"\\n\\n\")\n",
        "        print('predictions:',predictions, end=\"\\n\\n\")\n",
        "        print('actual value:',y[predictIndex], end=\"\\n\\n\")\n",
        "        print('return values ----------', end=\"\\n\\n\")\n",
        "        return mean, crossValidationResults['test_score'], predictions\n",
        "      else:\n",
        "        print('mean of cross validation test score:',mean, end=\"\\n\\n\")\n",
        "        print('cross validation test score:',crossValidationResults['test_score'], end=\"\\n\\n\")\n",
        "        print('return values ----------', end=\"\\n\\n\")\n",
        "        return mean, crossValidationResults['test_score']\n",
        "\n",
        "    elif cross is False:\n",
        "      regressor.fit(self.X, self.y)\n",
        "      prediction = regressor.predict([predict])\n",
        "      if predict is not None:\n",
        "        print('score of full data:',regressor.score(self.X, self.y), end=\"\\n\\n\")\n",
        "        print('prediction:', prediction, end=\"\\n\\n\")\n",
        "        print('actual value:',y[predictIndex], end=\"\\n\\n\")\n",
        "        print('return values ----------', end=\"\\n\\n\")\n",
        "        return regressor.score(self.X, self.y), prediction\n",
        "      else:\n",
        "        print('score of full data:',regressor.score(self.X, self.y), end=\"\\n\\n\")\n",
        "        print('return values ----------', end=\"\\n\\n\")\n",
        "        return regressor.score(self.X, self.y, end=\"\\n\\n\")\n",
        "\n",
        "  #Linear Regression\n",
        "  def LinearRegressor(self, cross, predict = None, predictIndex = None):\n",
        "    lrg = LinearRegression()\n",
        "    return self.common(lrg, cross, predict, predictIndex)\n",
        "\n",
        "  #ElasticNet Regression\n",
        "  def ElasticRegressor(self, cross, predict = None, predictIndex = None):\n",
        "    eln = ElasticNet(max_iter=2125)\n",
        "    return self.common(eln, cross, predict, predictIndex)\n",
        "\n",
        "  #Multi Task Elastic Regression\n",
        "  def MultiTaskElasticRegressor(self, cross, predict = None, predictIndex = None):\n",
        "    mte = linear_model.MultiTaskElasticNet(alpha=0.1, max_iter = 2000)\n",
        "    return self.common(mte, cross, predict, predictIndex)\n",
        "  \n",
        "  #Multi Task Lasso Regression\n",
        "  def MultiTaskLassoRegressor(self, cross, predict = None, predictIndex = None):\n",
        "    mtl = linear_model.MultiTaskLasso(alpha=0.1, max_iter=2000)\n",
        "    return self.common(mtl, cross, predict, predictIndex)\n",
        "\n",
        "  #Ridge Regeression\n",
        "  def RidRegressor(self, cross, predict = None, predictIndex = None):\n",
        "    rid = Ridge(alpha=1.0)\n",
        "    return self.common(rid, cross, predict, predictIndex)\n",
        "\n",
        "  #Lasso Regression\n",
        "  def LasRegressor(self, cross, predict = None, predictIndex = None):\n",
        "    las = linear_model.Lasso(alpha=0.1, max_iter=2000)\n",
        "    return self.common(las, cross, predict, predictIndex)\n",
        "\n",
        "  #KNeighbor Regression\n",
        "  def KNeighRegressor(self, cross, predict = None, predictIndex = None):\n",
        "    neigh = KNeighborsRegressor(n_neighbors=40)\n",
        "    return self.common(neigh, cross, predict, predictIndex)\n",
        "\n",
        "regressors = Regressor(X, y)\n",
        "regressors.MultiTaskElasticRegressor(cross=True, predict=X[2], predictIndex=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean of cross validation test score: 0.3703170871869467\n",
            "\n",
            "cross validation test score: [0.43407818 0.35480642 0.27694822 0.44954425 0.30628531 0.24354555\n",
            " 0.43150638 0.41392495 0.40313057 0.38940104]\n",
            "\n",
            "predictions: [[10.54889075 15.65601619 26.68079172]\n",
            " [ 9.55436866 15.79896773 28.28977027]\n",
            " [17.14575713 26.38577851 25.49440071]\n",
            " ...\n",
            " [16.89910219 27.05197709 24.58709307]\n",
            " [11.20646296 17.24517134 26.820145  ]\n",
            " [ 9.04889151 14.44490892 27.79371537]]\n",
            "\n",
            "actual value: [16.33 29.33 25.  ]\n",
            "\n",
            "return values ----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3703170871869467,\n",
              " array([0.43407818, 0.35480642, 0.27694822, 0.44954425, 0.30628531,\n",
              "        0.24354555, 0.43150638, 0.41392495, 0.40313057, 0.38940104]),\n",
              " array([[10.54889075, 15.65601619, 26.68079172],\n",
              "        [ 9.55436866, 15.79896773, 28.28977027],\n",
              "        [17.14575713, 26.38577851, 25.49440071],\n",
              "        ...,\n",
              "        [16.89910219, 27.05197709, 24.58709307],\n",
              "        [11.20646296, 17.24517134, 26.820145  ],\n",
              "        [ 9.04889151, 14.44490892, 27.79371537]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hmu6425Tx38"
      },
      "source": [
        "##**Model Comparisions & Selection**\n",
        "###**def RegressionSelector**\n",
        "This function create the dictionary each models' mean of test scores that are derived by 10-Fold Cross Validation.\n",
        "Also it creates the different mean of test scores by the data, **whether the missing features are filled by mean, median, or zero.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUr_Y575-1cU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecab7ae3-6150-47a6-f750-b9ea8445544c"
      },
      "source": [
        "def RegressionSelector():\n",
        "  dictionary = {}\n",
        "  for datatype in [\"Mean\", \"Zero\", \"Median\"]:\n",
        "    dictionary[datatype] = {}\n",
        "    if datatype == \"Mean\":\n",
        "      Data = FullData(MissingTask(df).Mean())\n",
        "    elif datatype == \"Zero\":\n",
        "      Data = FullData(MissingTask(df).Zero())\n",
        "    elif datatype == \"Median\":\n",
        "      Data = FullData(MissingTask(df).Median())\n",
        "    \n",
        "    X = Data.getVolumeArray('All')\n",
        "    y = Data.getScoreArray('All')\n",
        "\n",
        "    lrg = [LinearRegression(), 'linear']\n",
        "    eln = [ElasticNet(max_iter=2125), 'elastic net']\n",
        "    mte = [linear_model.MultiTaskElasticNet(alpha=0.1, max_iter = 2000), 'multi task elastic net']\n",
        "    rid = [Ridge(alpha=1.0), 'ridge']\n",
        "    las = [linear_model.Lasso(alpha=0.1, max_iter=2000), 'lasso']\n",
        "    mtl = [linear_model.MultiTaskLasso(alpha=0.1, max_iter=2000), 'multi task lasso']\n",
        "    neigh = [KNeighborsRegressor(n_neighbors=40), 'k neighbor']\n",
        "\n",
        "    models = [lrg, eln, mte, rid, las, mtl, neigh]\n",
        "\n",
        "    for model in models:\n",
        "      sum = 0\n",
        "      crossValidationResults = cross_validate(model[0], X, y, cv=kfold, scoring=(\"r2\"), return_train_score=True)\n",
        "      for index in range(10):\n",
        "        sum = sum + crossValidationResults['test_score'][index]\n",
        "      mean = sum / 10\n",
        "      dictionary[datatype][model[1]] = mean\n",
        "  return dictionary\n",
        "\n",
        "RegressionSelector()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Mean': {'elastic net': 0.28369656792203257,\n",
              "  'k neighbor': 0.15656783877861807,\n",
              "  'lasso': 0.35312534424768305,\n",
              "  'linear': 0.34961901498407255,\n",
              "  'multi task elastic net': 0.3703170871869467,\n",
              "  'multi task lasso': 0.3684817569638511,\n",
              "  'ridge': 0.3553057787909294},\n",
              " 'Median': {'elastic net': 0.28376987486753946,\n",
              "  'k neighbor': 0.15679314050856558,\n",
              "  'lasso': 0.35307853612971635,\n",
              "  'linear': 0.34950618715105325,\n",
              "  'multi task elastic net': 0.37026894538961974,\n",
              "  'multi task lasso': 0.3684371067360447,\n",
              "  'ridge': 0.3552137266852444},\n",
              " 'Zero': {'elastic net': 0.29742061536291925,\n",
              "  'k neighbor': 0.16763792715945564,\n",
              "  'lasso': 0.35994142574503807,\n",
              "  'linear': 0.35380031294849146,\n",
              "  'multi task elastic net': 0.3738753161956154,\n",
              "  'multi task lasso': 0.37314815849067295,\n",
              "  'ridge': 0.3595024221250281}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_A7smkN_Bc6"
      },
      "source": [
        "##**Comparision of the mean of the test scores by each models that are dervied by each different dataframes (Mean, Median, Zero)**\n",
        "\n",
        "As below, each test scores by 10 fold cross validation at the different data frames which are filled with ZERO, MEAN, MEDIAN.\n",
        "\n",
        "###**Mean**###\n",
        "At the dataframe whose missing values are filled by ***mean***, the **Multi Task Elastic Net** has the biggest accuracy of mean of 10 fold cross validation test score. \n",
        "> **Winner: Multi Task Elastic Net (0.3703170871869467)**\n",
        "\n",
        "###**Median**###\n",
        "At the dataframe whose missing values are filled by ***median***, the **Multi Task Elastic Net** has the biggest accuracy of median of 10 fold cross validation test score.\n",
        "> **Winner: Multi Task Elastic Net (0.37026894538961974)**\n",
        "\n",
        "###**Zero**###\n",
        "At the dataframe whose missing values are filled by ***zero***, the **Multi Task Elastic Net** has the biggest accuracy of zero of 10 fold cross validation test score.\n",
        "> **Winner: Multi Task Elastic Net (0.3738753161956154)**\n",
        "---\n",
        "##**The DataFrame whose missing values are filled with ZERO has the biggest results of test score**\n",
        "\n",
        "> ***Zero(0.3738753161956154) > Median(0.37026894538961974) > Mean(0.3703170871869467)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo0zuyUO_Hzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45733b10-8fa3-47d5-ac60-decf2b3f7059"
      },
      "source": [
        "mean = [0.28369656792203257, 0.15656783877861807, 0.35312534424768305, 0.34961901498407255, 0.3703170871869467, 0.3684817569638511, 0.3553057787909294]\n",
        "#1.Multi Task Elastic Net 2. Multi Task Lasso\n",
        "median = [0.28376987486753946, 0.15679314050856558, 0.35307853612971635, 0.34950618715105325, 0.37026894538961974, 0.3684371067360447, 0.3552137266852444]\n",
        "#1.Multi Task Elastic Net 2. Multi Task Lasso\n",
        "zero = [0.29742061536291925, 0.16763792715945564, 0.35994142574503807, 0.35380031294849146, 0.3738753161956154, 0.37314815849067295, 0.3595024221250281]\n",
        "#1.Multi Task Elastic Net 2. Multi Task Lasso\n",
        "\n",
        "#Zero > Mean > Median\n",
        "print(\"max of mean:\", np.max(mean))\n",
        "print(\"max of median:\", np.max(median))\n",
        "print(\"max of zero:\", np.max(zero))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max of mean: 0.3703170871869467\n",
            "max of median: 0.37026894538961974\n",
            "max of zero: 0.3738753161956154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXx0b6tCYct7"
      },
      "source": [
        "#**Persist the model by weights file and download**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZAvaTbZHDrl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b0045066-6f46-437d-8ab0-018ca9118abd"
      },
      "source": [
        "import joblib\n",
        "from google.colab import files\n",
        "\n",
        "Data = FullData(MissingTask(df).Zero())\n",
        "\n",
        "X = Data.getVolumeArray('All')\n",
        "yreg = Data.getScoreArray('All')\n",
        "yclf = Data.getDiagnosisArray()\n",
        "\n",
        "\n",
        "mte = linear_model.MultiTaskElasticNet(alpha=0.1, max_iter = 2000)\n",
        "mte.fit(X, yreg)\n",
        "cross_validate(mte, X, yreg, cv=kfold, scoring=('r2'), return_train_score=True)\n",
        "\n",
        "rfc = RandomForestClassifier(max_depth=40, random_state=0)\n",
        "rfc.fit(X, yclf)\n",
        "cross_validate(rfc, X, yclf, cv=kfold, return_train_score=True)\n",
        "\n",
        "joblib.dump(mte, 'elasticNet.pkl')\n",
        "joblib.dump(rfc, 'randomForest.pkl')\n",
        "\n",
        "#Regressor.predict([X[0]])\n",
        "#Classifier.predict([X[0]])\n",
        "\n",
        "files.download('elasticNet.pkl')\n",
        "files.download('randomForest.pkl')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a72fe171-3d49-46cd-9337-56f11a68423b\", \"elasticNet.pkl\", 4051)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8b4813a8-2742-44ba-b313-9fe16b6ce5ec\", \"randomForest.pkl\", 4247430)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZOehExrTXGd"
      },
      "source": [
        "---\n",
        "#**Analysis & Discussion**\n",
        "---\n",
        "> With the morphological phenotypes of brain from the data, there are Cortical Volume of 70 brain regions (ST102CV - ST99CV), and the Average Thickness (ST102TA - ST99TA) of 70 brain regions.\n",
        "> By classfiying the DX-bl which means the diagnosis group of 0: Cognitive Normal, 1: Mild Cognitive Normal, 2: Alzheimer's disease, the 70 regions of Cortical Volume and 70 regions of Average Thickness of each patient is showing slightly different aspects between each diagnosis group.\n",
        "\n",
        "##**Comparisions of the morphological phenotypes by each diagnosis group**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDuHvDApoToP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a6814d-4b7a-4500-c4ec-89471cd841b3"
      },
      "source": [
        "class0 = ClassData(Data.data, 0)\n",
        "class1 = ClassData(Data.data, 1)\n",
        "class2 = ClassData(Data.data, 2)\n",
        "\n",
        "print('Mean of Cortical Volume between classes',class0.MeanOfCV(), class1.MeanOfCV(), class2.MeanOfCV())\n",
        "print('Mean of Average Thickness between classes',class0.MeanOfAT(), class1.MeanOfAT(), class2.MeanOfAT())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of Cortical Volume between classes 6111.859006278583 6023.156696717224 5586.838403479871\n",
            "Mean of Average Thickness between classes 2.3929713185024424 2.33627246365299 2.2235102472960797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyZ-NwADnnsY"
      },
      "source": [
        "---\n",
        "> As Above, there are not large distictions between each diagnosis group with the Cortical Volume and Average Thickness of brain, **but still have a slightly difference with the amount of them**.\n",
        "\n",
        "**Comparision to mean of amount of Cortical Volume between diagnosis group** (Missing features were filled with mean of each column)\n",
        "\n",
        "> ***Cognitive Normal (6111.85) > Mild Cognitive Impariment (6023.15) > Alzheimer's disease (5586.83)***\n",
        "\n",
        "**Comparision to mean of amount of Average Thickness between diagnosis group**\n",
        "\n",
        "> ***Cognitive Normal (2.39) > Mild Cognitive Impariment (2.33) > Alzheimer's disease (2.22)***\n",
        "\n",
        "\n",
        "> As above comparision with the mean of amount by the each morphological phenotypes, which can be categorized to the Cortical Volume and Average Thickness, have a relation with determining the diagnosis group.\n",
        "---\n",
        "\n",
        "####**As the results,**\n",
        "\n",
        "> The amount of morphological phenotypes of Cortical Volume and Average Thickness for each diagnosis group are showing different aspects with the amount of each, cleary reveals that the amount of the Cortical Volume and the Average Thickness is getting smaller as the condition of brain gets worse. This shows that each of the morphological phenotypes that are Cortical Volume and Average Thickness are related to brain disease with the \"Alzheimer's\", as getting smaller of each 70 brain regions shows bad results to brain.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-sXoi10eaPr"
      },
      "source": [
        "##**Task 1 (3-class Classification): Predict the diagnosis group of subjects**\n",
        "---\n",
        "\n",
        "Like above, the diagnosis group of subjects can be specified with the amount of each Cortical Volume and Average Thickness of 70 regions of brain, by having slightly differences of amounts between diagnosis groups. The diagnosis group which indicates the label 'DX_bl' in the data can be predicted with the 70 brain regions of Cortical Volume and 70 brain regions of Average Thickness.\n",
        "\n",
        "Tasking the Classification, there were total 11 classification models to predict the diagnosis group of subjects, each of the shows different accuracy of predicting diagnosis group.\n",
        "\n",
        "---\n",
        "####**Models of Classification Task**\n",
        "\n",
        "- **Decision Tree Classifier**\n",
        "\n",
        "- **Gausian RBF Kernel SV Classifier**\n",
        "\n",
        "- **Perceptron Classifier**\n",
        "\n",
        "- **Polynomial Kernel SV Classifier**\n",
        "\n",
        "- **Neural Network Classifier**\n",
        "\n",
        "- **Logitic Regression Classifier**\n",
        "\n",
        "- **Linear SV Classifier <=> SVM (linear kernel)**\n",
        "\n",
        "- **Random Forest Classifier**\n",
        "\n",
        "- **Stochastic Gradient Descent Classifier**\n",
        "\n",
        "- **Support Vector Machine (linear kernel) Classifier**\n",
        "\n",
        "- **KNeighbor Classifier**\n",
        "\n",
        "---\n",
        "#####Each models gave the different accuracy at the three different data, which are filled with three different criteria (Mean, Median, Zero). Below table shows the mean of accuracy from 10 fold cross validation of each models from the different tasked data with the different criteria of Zero, Mean, Median.\n",
        "---\n",
        "\n",
        "|            \t| **Decision Tree** \t| **Gaussian RBF SVM** \t| **Linear SVC** \t| **Logistic** \t| **Neural Network** \t| **Perceptron** \t| **Polynomial SVM** \t| **Random Forest** \t|  **SGD**  \t|  **SVM**  \t| **K Neighbors** \t|\n",
        "|:----------:\t|:------------------:\t|:---------------------------:\t|:--------------:\t|:------------:\t|:------------------:\t|:--------------:\t|:-------------------------:\t|:-----------------:\t|:---------:\t|:---------:\t|:---------------:\t|\n",
        "|  **Mean**  \t|      0.4617546     \t|          0.5065062          \t|    0.5028454   \t|   0.5108928  \t|      0.5065062     \t|    0.4647399   \t|         0.5221467         \t|     0.5494836     \t| 0.4786413 \t| 0.5470147 \t|    0.4872437    \t|\n",
        "|  **Zero**  \t|      0.4536801     \t|          0.5065062          \t|    0.5040683   \t|   0.5059549  \t|      0.5065062     \t|    0.4536762   \t|         0.5239868         \t|     0.5544487     \t| 0.4952989 \t| 0.5420225 \t|    0.4841187    \t|\n",
        "| **Median** \t|      0.4586413     \t|          0.5065062          \t|    0.5034666   \t|   0.5096506  \t|      0.5065062     \t|    0.4268827   \t|         0.5215256         \t|     0.5457453     \t| 0.4816459 \t| 0.5476397 \t|    0.4866187    \t|\n",
        "|   **MAX**  \t|      0.4617546     \t|          0.5065062          \t|    0.5040683   \t|   0.5108928  \t|      0.5065062     \t|    0.4647399   \t|         0.5239868         \t|     0.5544487     \t| 0.5034239 \t| 0.5476397 \t|    0.4872437    \t|\n",
        "\n",
        "---\n",
        "As above, at the three different dataset whose missing features are filled with different cirteria (Zero, Mean, Median), each model shows different aspects of test scores. \n",
        "Among of them, **the Random Forest showed the largest test score** from the **Zero-filled** dataset,the score of 0.5544487. \n",
        "#####The model that showed second largest score was the **SVM (Linear Kernel)** about 0.5420225 of score at the Zero-filled data, the first largest score at the Median-filled data with the score of 0.5476397.\n",
        "---\n",
        "\n",
        "#####**Comparision of test scores between models at Zero-Filled Data**\n",
        "\n",
        "> ***Random Forest (0.5544487) > SVM (Linear Kernel) (0.5420225) > SVM (Poly Kernel) (0.5239868)***\n",
        "\n",
        "#####**Comparision of test scores between models at Mean-Filled Data**\n",
        "\n",
        "> ***Random Forest (0.5494836) > SVM (Linear Kernel) (0.5470147) > SVM (Poly Kernel) (0.5221467)***\n",
        "\n",
        "#####**Comparision of test scores between models at Median-Filled Data**\n",
        "\n",
        "> ***SVM (Linear Kernel) (0.5476397) > Random Forest (0.5457453) >  SVM (Poly Kernel) (0.5215256)***\n",
        "\n",
        "---\n",
        "\n",
        "#####**As the results,** **the Random Forest** model at the data whose missing features are filled with **the Zero** has the largest accuracy beyond other models at the three different filled-data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4A0My29ehan"
      },
      "source": [
        "##**Task 2 (3-logit Regression): Predict the cognitive assessment scores of subjects**\n",
        "---\n",
        "\n",
        "With the regression task to predict the cognitive assessment scores of subjects which are ADAS11, ADAS13, MMSE, each assessment scores of them shows the higher score leads to the normal type of disease, while the lower score of each of them indicates disease like Alzheimer's.\n",
        "\n",
        "Tasking of regression, there were total 7 regression tasks to predict the cognitive assessment scores of subject, each of them shows different accuracy of predicting scores.\n",
        "\n",
        "---\n",
        "\n",
        "####**Models of Regression task**\n",
        "\n",
        "- **Linear Regression**\n",
        "\n",
        "- **Lasso Regression**\n",
        "\n",
        "- **Multi Task Lasso Regression**\n",
        "\n",
        "- **Ridge Regression**\n",
        "\n",
        "- **Elastic Net Regression**\n",
        "\n",
        "- **Multi Task Elastic Net Regression**\n",
        "\n",
        "- **KNeighbors Regression**\n",
        "\n",
        "---\n",
        "As the classification models above do, each models gave the different accuracy at the three different data, which are filled with three different criteria (Mean, Median, Zero). Below table shows the mean of accuracy from 10 fold cross validation of each models from the different tasked data with the different criteria of Zero, Mean, Median.\n",
        "\n",
        "---\n",
        "\n",
        "|            \t| **Elastic Net** \t| **Multi Task Elastic Net** \t| **K Neighbors** \t| **Lasso** \t| **Linear** \t| **Multi Task Lasso** \t| **Ridge** \t|\n",
        "|:----------:\t|:---------------:\t|:--------------------------:\t|:---------------:\t|:---------:\t|:----------:\t|:--------------------:\t|-----------\t|\n",
        "|  **Mean**  \t|    0.3703170    \t|          0.3703170         \t|    0.1565678    \t| 0.3531253 \t|  0.3496190 \t|       0.3684817      \t| 0.3553057 \t|\n",
        "|  **Zero**  \t|    0.2974206    \t|          0.3738753         \t|    0.1676379    \t| 0.3599414 \t|  0.3538003 \t|       0.3731481      \t| 0.3595024 \t|\n",
        "| **Median** \t|    0.28376987   \t|          0.3702689         \t|    0.1567931    \t| 0.3530785 \t|  0.3495061 \t|       0.3684371      \t| 0.3552137 \t|\n",
        "|   **MAX**  \t|    0.3703170    \t|          0.3738753         \t|    0.1676379    \t| 0.3599414 \t|  0.3538003 \t|       0.3731481      \t| 0.3595024 \t|\n",
        "\n",
        "---\n",
        "\n",
        "As above, at the three different dataset whose missing features are filled with different cirteria (Zero, Mean, Median), each model shows different aspects of test scores. \n",
        "Among of them, **Multi task Elastic Net showed the largest test score** from the **Zero-filled** dataset,the score of 0.3738753. The Multi task Elastic Net won at all 3 different data. \n",
        "#####The model that showed second largest score was the **Lasso** about 0.3738753 of score at the Zero-filled data.\n",
        "---\n",
        "\n",
        "#####**Comparision of test scores between models at Zero-Filled Data**\n",
        "\n",
        "> ***Multi Task Elastic Net (0.3738753) > Multi Task Lasso (0.3731481) > Lasso (0.3599414)***\n",
        "\n",
        "#####**Comparision of test scores between models at Median-Filled Data**\n",
        "\n",
        "> ***Multi Task Elastic Net (0.3702689) >= Elastic Net (0.3702689) > Multi Task Lasso (0.3684371)***\n",
        "\n",
        "#####**Comparision of test scores between models at Mean-Filled Data**\n",
        "\n",
        "> ***Multi Task Elastic Net (0.3703170) > Multi Task Lasso (0.3684817) >  Ridge (0.3553057)***\n",
        "\n",
        "---\n",
        "\n",
        "#####**As the results,** **the Multi Task Elastic Net** model at the data whose missing features are filled with **the Zero** has the largest accuracy beyond other models at the three different filled-data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adJN74O6QC7I"
      },
      "source": [
        "##**Summary**\n",
        "Since the morophological phenotypes, which are Cortical Volume and Average Thickness of 70 brain regions are related to the degree of cognitive disease, predicting the diagnosis group and assessment scores could be derived with the data of 140 columns from dataframe.\n",
        "\n",
        "With the 3 different dataframe, whose missing features are filled with three different criteria (Mean, Median, Zero), they derived slightly different results of accuracy with both regression and classification task.\n",
        "\n",
        "Both classifciation and regression task brought the same results that **the Zero-filled missing features data had more accuracy** than the other data such as Mean-filled and Median-filled.\n",
        "\n",
        "**At the classification task, the Random Forest classifier** at the Zero-filled data, with the 10-fold-cross validation gave the best test_score, the accuracy of 0.5544487.\n",
        "\n",
        "**At the regression task, the Multi Task Elastic Net Regression** at the Zero-filled data, with the 10-fold-cross validation gave the best test_score, the accuracy of 0.3738753."
      ]
    }
  ]
}